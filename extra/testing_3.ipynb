{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ffdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from quickgrad.engine import Number\n",
    "from quickgrad.nn import MLP, Layer, Neuron, Module, create_tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ce5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class vector:\n",
    "#     def __init__(self, data, _children=(), _op='', label=''):\n",
    "#         self.data = np.array(data)\n",
    "#         self.grad = np.zeros_like(data)\n",
    "#         self.label = label\n",
    "#         self._backward = lambda : None\n",
    "#         self._prev = set(_children)\n",
    "#         self._op = _op\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return f\"vector(data={self.data})\"\n",
    "\n",
    "#     def __add__(self, other):\n",
    "#         # assert self.s==len(other), \"should be same shape\"\n",
    "#         other = other if isinstance(other, vector) else vector(other)\n",
    "#         out = vector(self.data + other.data, (self, other), '+')\n",
    "\n",
    "#         def _backward():\n",
    "#             self.grad += 1.0 * out.grad\n",
    "#             other.grad += 1.0 * out.grad\n",
    "#         out._backward = _backward\n",
    "#         return out\n",
    "\n",
    "#     def exp(self):\n",
    "#         # print(math.exp(self.data))\n",
    "#         out = vector(np.exp(self.data), (self,), 'exp')\n",
    "\n",
    "#         def _backward():\n",
    "#             self.grad += out.data * out.grad\n",
    "#         out._backward = _backward\n",
    "#         return out\n",
    "\n",
    "#     def log(self):\n",
    "#         out = vector(np.log(self.data), (self,), 'log')\n",
    "\n",
    "#         def _backward():\n",
    "#             self.grad += (1 / self.data ) * out.grad\n",
    "#         out._backward = _backward\n",
    "#         return out\n",
    "\n",
    "#     def __pow__(self, other):\n",
    "#         other = other if isinstance(other, Number) else Number(other)\n",
    "#         out = vector(np.power(self.data, other.data), (self, other), '_pow')\n",
    "\n",
    "#         def _backward():\n",
    "#             self.grad += (other.data * (self.data ** (other.data -1))) * out.grad\n",
    "#         out._backward = _backward\n",
    "#         return out\n",
    "    \n",
    "#     def sum(self, axis=None, keepdims=True):\n",
    "#         out = np.sum(self.data, axis=axis, keepdims=keepdims)\n",
    "#         out = vector(out, (self,), 'sum')\n",
    "\n",
    "#         def _backward():\n",
    "#             self.grad += np.ones_like(self.data, dtype=np.float32) * out.grad\n",
    "#         out._backward = _backward\n",
    "#         return out\n",
    "    \n",
    "#     def __mul__(self, other):\n",
    "#         other = other if isinstance(other, Number) else vector(other)\n",
    "#         out = vector(self.data * other.data, (self, other), '*')\n",
    "\n",
    "#         def _backward():\n",
    "#             self.grad += other.data * out.grad\n",
    "#         out._backward = _backward\n",
    "#         return out\n",
    "    \n",
    "#     def broadcast_to(self, target_shape):\n",
    "#         input_shape = self.shape()\n",
    "#         out = vector(np.broadcast_to(self, target_shape), (self,), 'broadcast')\n",
    "\n",
    "#         def _backward():\n",
    "#             broadcast_axes = get_broadcast_axes(input_shape, target_shape)[0]\n",
    "#             ans = out.grad\n",
    "#             print(broadcast_axes)\n",
    "#             print(ans)\n",
    "#             new_ans = ans.sum(broadcast_axes, keepdims=True)\n",
    "#             print(vector(new_ans).shape())\n",
    "#             print(vector(self.grad).shape())\n",
    "#             self.grad += vector(ans.sum(broadcast_axes, keepdims=True)) * out.grad\n",
    "\n",
    "#         # def _backward():\n",
    "#         #     broadcast_axes = get_broadcast_axes(input_shape, target_shape)[0]\n",
    "#         #     ans = out.grad\n",
    "#         #     print(broadcast_axes)\n",
    "#         #     print(ans)\n",
    "#         #     new_ans = ans.sum(broadcast_axes, keepdims=True)\n",
    "#         #     print(vector(new_ans).shape())\n",
    "#         #     print(vector(self.grad).shape())\n",
    "#         #     self.grad += vector(ans.sum(broadcast_axes, keepdims=True))\n",
    "#         out._backward = _backward\n",
    "#         return out\n",
    "\n",
    "    \n",
    "        \n",
    "#     def mean(self, axis=None):\n",
    "#         n = self.shape()[axis]\n",
    "#         out = self.sum(axis, keepdims=False) * (n ** -1)\n",
    "#         return out\n",
    "\n",
    "#     # def std(self, axis):\n",
    "#     #     # mean = vector(self.mean(axis))\n",
    "#     #     mean = self.mean(axis)\n",
    "#     #     print(mean)\n",
    "#     #     ans = self - mean\n",
    "#     #     print(ans.shape())\n",
    "#     #     print(ans)\n",
    "#     #     numer = ans.sum(axis=axis, keepdims=False)\n",
    "#     #     denom = self.shape()[axis] - 1\n",
    "#     #     # print(numer.shape())\n",
    "#     #     # print(denom)\n",
    "#     #     out = numer / denom\n",
    "#     #     return out\n",
    "        \n",
    "#     def __neg__(self):\n",
    "#         return self * -1\n",
    "    \n",
    "#     def __sub__(self, other):\n",
    "#         return self + (- other)\n",
    "    \n",
    "#     def __rsub__(self, other):\n",
    "#         return other - self\n",
    "    \n",
    "#     def __radd__(self, other):\n",
    "#         return - self + other\n",
    "    \n",
    "#     def __truediv__(self, other):\n",
    "#         return self * (other ** -1)\n",
    "    \n",
    "#     def __rtruediv__(self, other):\n",
    "#         return (self ** -1) * (other)\n",
    "\n",
    "#     def shape(self):\n",
    "#         return np.shape(self.data)\n",
    "    \n",
    "#     def backward(self):\n",
    "#         topo = []\n",
    "#         visited = set()\n",
    "#         def build_topo(v):\n",
    "#             if v not in visited:\n",
    "#                 visited.add(v)\n",
    "#                 for child in v._prev:\n",
    "#                     build_topo(child)\n",
    "#                 topo.append(v)\n",
    "#         build_topo(self)\n",
    "        \n",
    "#         self.grad = vector(np.ones_like(self.data))\n",
    "#         for node in reversed(topo):\n",
    "#             node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0965e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class vector:\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float64)  # Keep as numpy array\n",
    "        self.label = label\n",
    "        self._backward = lambda : None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"vector(data={self.data})\"\n",
    "\n",
    "    # Add __iadd__ to support += operations\n",
    "    def __iadd__(self, other):\n",
    "        if isinstance(other, vector):\n",
    "            self.data += other.data\n",
    "        else:\n",
    "            self.data += other\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, vector) else vector(other)\n",
    "        out = vector(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad +=  out.grad\n",
    "            other.grad +=  out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        out = vector(np.exp(self.data), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "        out = vector(np.log(self.data), (self,), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            other_data = other\n",
    "        elif hasattr(other, 'data'):\n",
    "            other_data = other.data\n",
    "        else:\n",
    "            other_data = other\n",
    "            \n",
    "        out = vector(np.power(self.data, other_data), (self,), '_pow')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other_data * (self.data ** (other_data - 1))) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None, keepdims=True):\n",
    "        out = np.sum(self.data, axis=axis, keepdims=keepdims)\n",
    "        out = vector(out, (self,), 'sum')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += np.ones_like(self.data, dtype=np.float64) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, vector) else vector(other)\n",
    "        other = other.broadcast_to(self.shape())\n",
    "        out = vector(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad \n",
    "            other.grad += self.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def broadcast_to(self, target_shape):\n",
    "        input_shape = self.shape()\n",
    "        out = vector(np.broadcast_to(self.data, target_shape), (self,), 'broadcast')\n",
    "\n",
    "        def _backward():\n",
    "            broadcast_axes_result = get_broadcast_axes(input_shape, target_shape)\n",
    "            broadcast_axes = broadcast_axes_result[0]\n",
    "    \n",
    "            if broadcast_axes and isinstance(broadcast_axes[0], list):\n",
    "                broadcast_axes = broadcast_axes[0]\n",
    "            ans = out.grad\n",
    "            reduced_grad = ans\n",
    "            \n",
    "            if broadcast_axes:  \n",
    "                axes_tuple = tuple(broadcast_axes)\n",
    "                reduced_grad = np.sum(ans, axis=axes_tuple, keepdims=True)\n",
    "                reduced_grad = reduced_grad.reshape(input_shape)\n",
    "            self.grad += reduced_grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def expand_dims(self, axis):\n",
    "        return vector(np.expand_dims(self, axis=axis))\n",
    "    \n",
    "    def transpose(self, axis):\n",
    "        out = vector(np.transpose(self.data, axes=axis), (self,), 'T')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += np.transpose(out.grad, axes=axis)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def var(self, axis):\n",
    "        mean = self.mean(axis)    \n",
    "        new_ans = mean.broadcast_to(self.shape())\n",
    "        ans = (self - mean) ** 2\n",
    "        numer = ans.sum(axis=axis, keepdims=False)\n",
    "        denom = self.shape()[axis] \n",
    "        out = (numer / denom)\n",
    "        return out \n",
    "\n",
    "    def std(self, axis):\n",
    "        return self.var(axis) ** (1/2)\n",
    "        \n",
    "    def mean(self, axis=None):\n",
    "        n = self.shape()[axis] if axis is not None else self.data.size\n",
    "        out = self.sum(axis, keepdims=True) * (n ** -1)\n",
    "        return out\n",
    "\n",
    "    def reshape(self, target_shape):\n",
    "        out = vector(np.reshape(self.data, target_shape))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad.reshape(self.data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __setitem__(self, index, val):\n",
    "        self.data[index] = val.data.copy()\n",
    "        self.grad[index] = val.grad.copy()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        out = vector(self.data[index], (self,), 'get_item')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad[index] += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # def split(self, indices, axis):\n",
    "    #     out = vector(np.split(self, indices_or_sections=indices, axis=axis))\n",
    "\n",
    "    #     def _backward():\n",
    "    #         grad_pieces = [piece.grad for piece in out]\n",
    "    #         self.grad += np.concatenate(grad_pieces, axis=axis)\n",
    "\n",
    "    #     out._backward = _backward\n",
    "    #     return out\n",
    "\n",
    "    def concat(self, other, axis):\n",
    "        self_shape = self.shape()\n",
    "        axis_self_shape = self_shape[axis]\n",
    "        out = vector(np.concatenate((self.data, other.data), axis=axis), (self, other), 'cat')\n",
    "\n",
    "        def _backward():\n",
    "            self_grad, other_grad = np.array_split(out.grad, [axis_self_shape], axis=axis)\n",
    "            self.grad += self_grad\n",
    "            other.grad += other_grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # def __2dmatmul__(self, other):\n",
    "    #     out = vector(np.matmul(self.data, other.data), (self, other), '@')\n",
    "\n",
    "    #     def _backward():\n",
    "    #         other_data = other.transpose((1,0)).data\n",
    "    #         self_data = self.transpose((1,0)).data\n",
    "    #         self.grad += np.matmul(out.grad, other_data)\n",
    "    #         other.grad += np.matmul(self_data, out.grad)\n",
    "\n",
    "    #     out._backward = _backward\n",
    "    #     return out\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        out = vector(np.matmul(self.data, other.data), (self, other), '@')\n",
    "        \n",
    "        def _backward():        \n",
    "            other_transposed = np.swapaxes(other.data, -2, -1)\n",
    "            self_transposed = np.swapaxes(self.data, -2, -1)\n",
    "            \n",
    "            self.grad += np.matmul(out.grad, other_transposed)\n",
    "            other.grad += np.matmul(self_transposed, out.grad)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def masked_fill(self, condition, val):\n",
    "        out = vector(np.where(condition, self.data, val.data))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += np.where(condition, 0, out.grad)\n",
    "            val.grad += np.where(condition, out.grad, 0)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # def clip(self, min_val, max_val):\n",
    "    #     out = vector(min(max(self.data, min_val), max_val))\n",
    "\n",
    "    #     def _backward():\n",
    "    #         self.grad = np.clip(self.grad, min_val, max_val)\n",
    "    #     out._backward = _backward\n",
    "    #     return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return vector(other) + (-self)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return vector(other) + self\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other ** -1)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return vector(other) * (self ** -1)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        # FIXED: Keep grad as numpy array, not vector\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "# Helper function (you'll need to implement this based on your needs)\n",
    "def get_broadcast_axes(input_shape, target_shape):\n",
    "    \"\"\"\n",
    "    Returns the axes that were broadcasted from input_shape to target_shape\n",
    "    \"\"\"\n",
    "    # Simple implementation - you may need to adjust based on your specific needs\n",
    "    input_ndim = len(input_shape)\n",
    "    target_ndim = len(target_shape)\n",
    "    \n",
    "    # Pad input_shape with 1s at the beginning if needed\n",
    "    padded_input = (1,) * (target_ndim - input_ndim) + input_shape\n",
    "    \n",
    "    broadcast_axes = []\n",
    "    for i, (inp_dim, tgt_dim) in enumerate(zip(padded_input, target_shape)):\n",
    "        if inp_dim == 1 and tgt_dim > 1:\n",
    "            broadcast_axes.append(i)\n",
    "    \n",
    "    return [broadcast_axes], []  # Return in expected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5f8cb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = np.random.randn(2, 1, 2)\n",
    "# bb = np.random.randn(2, 2, 1)\n",
    "# a = vector(aa)\n",
    "# b = vector(bb)\n",
    "\n",
    "a = vector(np.array([[1, 2],      # shape (2, 3)\n",
    "     [3, 4]]))\n",
    "b = vector(np.array([[5, 6],      # shape (2, 2) \n",
    "     [7, 8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "7568bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.__matmul__(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bfa1ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2),\n",
       " vector(data=[[19 22]\n",
       "  [43 50]]))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape(), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f613755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c6f815d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e724657d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[11., 15.],\n",
       "        [11., 15.]]),\n",
       " array([[4., 4.],\n",
       "        [6., 6.]]))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b0779e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.arange(9)\n",
    "a = vector(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7228aea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.linalg' has no attribute 'matmul'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.linalg' has no attribute 'matmul'"
     ]
    }
   ],
   "source": [
    "np.linalg.matmul(np.random.randn(3, 1), np.random.randn(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "91436d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(10, 3, 4)\n",
    "b = np.random.randn(10, 4, 5)\n",
    "\n",
    "# result shape: (10, 3, 5)\n",
    "result = np.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "afe438a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,   0.],\n",
       "        [-inf, -inf]]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.random.randint(0, 2, (2,2)),\n",
    "np.where(aa, np.array(0), -float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4dea09eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(aa, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "20acb5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1],\n",
       "        [0, 0]]),)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "699b2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.random.randn(2, 3)\n",
    "bb = np.random.randn(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e8f8b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vector(aa)\n",
    "b = vector(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "540d34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.__2dmatmul__(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8845e753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vector(data=[[ 0.76828079 -0.03428902 -0.33034202]\n",
       " [-2.1442991  -0.65824457 -0.32136415]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "684efa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector(data=[[-0.48954855  1.18329686]\n",
      " [ 0.31236722  0.31378672]\n",
      " [-0.31682724 -0.81748092]])\n"
     ]
    }
   ],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e37a9e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69374831,  0.62615394, -1.13430817],\n",
       "       [ 0.69374831,  0.62615394, -1.13430817]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0881da87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.37601831, -1.37601831],\n",
       "       [-0.69253359, -0.69253359],\n",
       "       [-0.65170616, -0.65170616]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7279f2e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'vector' and 'vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;129;43m@b\u001b[39;49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for @: 'vector' and 'vector'"
     ]
    }
   ],
   "source": [
    "a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector(aa)@2dvector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d01bde80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)"
     ]
    }
   ],
   "source": [
    "np.matmul(np.random.randn(3, 1), np.random.randn(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b99f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
